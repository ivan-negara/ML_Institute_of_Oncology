---
title: "timeseriesML"
output: html_document
date: "2023-12-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Data Cleaning
```{r 1, echo=FALSE}
#remove TUMORI CAP SI GAT - outliers.
# 3 data points for aml - outliers
library(fs)
library(corrplot)
library(purrr)
library(gridExtra)
library(h2o)
library(xgboost)
library(ranger)
library(psych)
library(reshape2)
library(ggplot2)
library(dplyr)
library(tidyr)
library(readxl)
library(plotly)
library(tidyquant)
library(colorspace)
library(RColorBrewer)
library(tidyverse)
library(lubridate)
library(writexl)
library(googledrive)
library(googlesheets4)
library(jsonlite)
library(htmlwidgets)
library(xts)
library(TSstudio)
library(plotly)
library(workflows)
library(parsnip)
library(recipes)
library(yardstick)
library(glmnet)
library(tidyverse)
library(tidyquant)
library(timetk) # Use >= 0.1.3, remotes::install_github("business-science/timetk")
library(rsample)
library(modeltime)
library(vip)
#NOTE THAT email is set after first authentication
set.seed(1975)
gs4_auth(email = "ivan.negara1996@gmail.com") 
drive_auth(email = "ivan.negara1996@gmail.com") 

drive_download("https://docs.google.com/spreadsheets/d/1C-7l6nkjKGJ_gXUnnzuYvE2qlhUuMAvZ/edit#gid=189902746", overwrite = TRUE, path = "C:/Users/user/Desktop/Stat/Cancer/Rand_file.xlsx")

#new data
df_spit <- read_sheet("https://docs.google.com/spreadsheets/d/13hKJmUsPDoib3FP6yRNJpXn0eLvxGjPXg2dUUsYobaM/edit?resourcekey#gid=410176990", sheet = 1)

#remove first col
df_spit<- df_spit[,-1]

#rename
df_spit <- df_spit %>%
  dplyr::rename(New_date = `Data prevazuta de asteptare a spitalizarii`, Sectie = `Denumirea secției`, Data = `Data actuala`)

#calculate and mutate
df_spit <- df_spit %>%
  mutate(Zile = difftime(New_date, Data, units= "days"))
df_spit$Zile <- as.numeric(df_spit$Zile)

#remove unneeded date
df_spit<- df_spit[,-3]

#convert data to date
df_spit <- df_spit %>%
  mutate_at(vars(starts_with("Dat")), as.Date)

#existing data
RowCan <- read_xlsx("C:/Users/user/Desktop/Stat/Cancer/Rand_file.xlsx", sheet = 1)

#remove second column empty
RowCan <- RowCan[,-2]

# Convert the selected columns to Date format
RowCan <- RowCan %>%
  mutate_at(vars(starts_with("Dat")), as.Date)


#  remove 44, long data

Row_final <- RowCan %>%
  select(-starts_with("44"))

Row_final <- Row_final %>%
  dplyr::rename(Sectie = `Denumirea secției`)

Row_long <- Row_final %>%
  pivot_longer(
    cols = matches("Data\\.\\.\\..*|Nr\\. Zile\\.\\.\\..*"), 
    names_to = c(".value", "group"), 
    names_pattern = "(Data|Nr\\. Zile)\\.\\.\\.(\\d+)"
  )

Row_long <- Row_long %>%
  select(-starts_with("45"))
Row_long <- Row_long %>% select(-group)


#final data
new_df <- data.frame(Sectie = Row_long$Sectie[seq(1, nrow(Row_long), by = 2)], Data = Row_long$Data[seq(1, nrow(Row_long), by = 2)], Zile = Row_long$`Nr. Zile`[seq(2, nrow(Row_long), by = 2)])

Spital <- rbind(new_df, df_spit)

#Impute NAs - FINAL DATA FOR WEEKS
imputed_spital <- Spital %>%
  group_by(Sectie) %>%
  fill(Zile, .direction = "up")

#final data for MONTHS

#add month-year
imputed_spital2 <- imputed_spital 
imputed_spital2$Month <- format(imputed_spital$Data, format = "%Y-%m")
  

#calculate mean for every month
mean_spital <- imputed_spital2 %>%
 group_by(Sectie, Month) %>%
  summarise(`Mean Days` = round(mean(Zile),1))



googlesheets4::gs4_deauth()
drive_deauth()
```
### time series preprocess
```{r 2, echo=FALSE}
library(tseries)
library(forecast)
library(remotes)
library(tsibble)
library(lessR)
library(fpp3)
### Function to subset data for a specific department and convert to xts - THINK THAT BETTER CODE IS BELOW
# subset_xts <- function(data, sectie) {
#   # Subset data for the specified department
#   subset_data <- data[data$Sectie == sectie, ]
#   
#   # Convert to xts and ensure data is ordered by time
#   xts_data <- as.xts(subset_data)
#   
#   return(xts_data)
# }
# 
# # Subset by sectie
# dep_gastr <- subset_xts(imputed_spital, "Gastrologie")
# dep_gastr <- dep_gastr[, !colnames(dep_gastr) %in% c("Sectie")]
# 
# mean_gastr<-period.apply(dep_gastr, INDEX = endpoints(dep_gastr, on = "months"), FUN = mean)

#create ts and tsibble object for all sectii
imputed_spital <- imputed_spital %>%
  dplyr::rename(Date = Data, Department = Sectie, Zile = Zile)



#remove duplicates
key_columns <- c("Department")
index_column <- "Date"
# Get the row numbers
row_numbers <- which(is_duplicate <- duplicated(imputed_spital %>% select(all_of(key_columns), all_of(index_column))) | 
          duplicated(imputed_spital %>% select(all_of(key_columns), all_of(index_column)), fromLast = TRUE))
imputed_spital <- imputed_spital[-row_numbers, ]

#create tsibble and xts

df <- as.xts(imputed_spital)
df_tsibble <- as_tsibble(imputed_spital,  index = Date, key = Department)


# Make data regularly spaced by moving every date to friday
df_standardized_date <- df_tsibble %>%
 mutate(new_date = floor_date(Date, "week") + days(5))


# Step 2: Fill in missing weeks
df_tsibble_filled <- df_standardized_date %>%
   complete(new_date = seq(min(new_date), max(new_date), by = "1 week"), fill = list(Value = 0))
df_tsibble_filled <- df_tsibble_filled %>%
  mutate(Date = new_date) %>%
  select(-new_date)

  #interpolate everything in between, remove duplicates
row_numbers <- which(is_duplicate <- duplicated(df_tsibble_filled %>% select(all_of(key_columns), all_of(index_column))) | 
          duplicated(df_tsibble_filled %>% select(all_of(key_columns), all_of(index_column)), fromLast = TRUE))
df_tsibble_filled <- df_tsibble_filled[-row_numbers, ]
df_standardized_date2 <- df_tsibble_filled %>%
 mutate(new_date = floor_date(Date, "week") + days(5))
df_tsibble_filled2 <- df_standardized_date2 %>%
   complete(new_date = seq(min(new_date), max(new_date), by = "1 week"), fill = list(Value = 0))
df_tsibble_filled2 <- df_tsibble_filled2 %>%
  mutate(Date = new_date) %>%
  select(-new_date)

df_tsibble_filled2 <- df_tsibble_filled2 %>% arrange(Department, Date)

# Group by 'Sectie' and apply na.approx within each group
df_tsibble_interpolated <- df_tsibble_filled2 %>%
  group_by(Department) %>%
  mutate(Zile = zoo::na.approx(Zile, na.rm = FALSE)) %>%
  ungroup()

df_tsibble_interpolated <- df_tsibble_interpolated %>% mutate(Zile = round(Zile, 0))

#####deal with gyneco

# Identify the split date
split_date <- as.Date("2023-12-15")

# Separate data into two parts
df_before_split <- df_tsibble_interpolated[df_tsibble_interpolated$Date < split_date, ]
df_after_split <- df_tsibble_interpolated[df_tsibble_interpolated$Date >= split_date, ]

# Calculate the mean of Gynecology benign and malignant for 'Sectie'
mean_data <- df_after_split %>%
  group_by(Date) %>%
  filter(Department %in% c("Ginecologie, Benign", "Ginecologie, Malign")) %>%
  summarize(Zile = mean(Zile, na.rm = TRUE))
mean_data$Date <- as.Date(mean_data$Date, format="%Y-%m-%d")
mean_data <- mean_data %>% mutate(Department = "Ginecologie")
# Assuming df_tsibble_interpolated is your main data frame
# Assuming mean_data is the data frame with Date and Zile values

# Filter data for Ginecologie
ginecologie_data <- df_tsibble_interpolated[df_tsibble_interpolated$Department == "Ginecologie", ]
ginecologie_data$Date <- as.Date(ginecologie_data$Date, format="%Y-%m-%d")

# Merge the ginecologie_data with mean_data based on Date
ginecologie_data <- rbind(ginecologie_data, mean_data)

# Round Zile
ginecologie_data$Zile <- round(ginecologie_data$Zile, 0)
ginecologie_data <- filter(ginecologie_data, Date >= "2023-12-15")

# Replace the values in df_tsibble_interpolated with the modified Ginecologie data
merged_data <- rbind(df_tsibble_interpolated, ginecologie_data)
#remove benign and malignant ginecologie
merged_data <- merged_data %>% filter(Department != "Ginecologie, Benign", Department != "Ginecologie, Malign")

merged_data %>% arrange(Department, Date)

df_tsibble_interpolated <- merged_data

# #rename departments
df_tsibble_interpolated_renamed <-  as.data.frame(df_tsibble_interpolated)  %>%
 mutate(Department = case_when(
        Department == "Ginecologie" ~ "Gynecology",
        Department == "Tumori ale pielii, melanom și ALM" ~ "Dermato-Oncology",
        Department == "Broncoscopie Chirurugie de o zi"   ~ "Bronchoscopy", 
        Department == "Gastrologie" ~ "Gastroenterology",
        Department == "Hematologie 1" ~ "Hematology 1",
        Department == "Hematologie 2" ~ "Hematology 2",
        Department == "Hematologie 4" ~ "Hematology 3",
        Department == "Hematologie pentru copii" ~ "Pediatric Hematology",
        Department == "Mamologie, Benign" ~ "Breast Oncosurgery, benign",
        Department == "Mamologie, Malign Primar" ~ "Breast Oncosurgery, primary malignant",
        Department == "Mamologie, Malign Secundar" ~ "Breast Oncosurgery, secondary malignant",
        Department == "Oncologie medicală 1" ~ "Medical Oncology 1",
        Department == "Oncologie medicală 2" ~ "Medical Oncology 2",
        Department == "Oncologie medicală 3" ~ "Medical Oncology 3",
        Department == "Oncologie radiologică 1" ~ "Radiation Oncology 1",
        Department == "Oncologie radiologică 2" ~ "Radiation Oncology 2",
        Department == "Oncologie radiologică 3" ~ "Radiation Oncology 3",
        Department == "Oncologie radiologică 4/Branhiterapia" ~ "Brachytherapy",
        Department == "Pediatrie oncologică" ~ "Pediatric Oncology",
        Department == "Proctologie" ~ "Proctology",
        Department == "Staționar de zi" ~ "Day hospital",
        Department == "Urologie" ~ "Urology",
        Department == "Toraco-abdominală" ~ "Thoracoabdominal oncology",
        Department == "Tumori cap și gât, Anestezie generală" ~ "Head and Neck Tumours, general anaesthesia",
        Department == "Tumori cap și gât, Anestezie locală" ~ "Head and Neck Tumours, local anaesthesia",
        Department == "Tumori cap și gât, Neonco" ~ "Head and Neck Tumours, benign",
        ))
df_tsibble_interpolated <- as_tsibble(df_tsibble_interpolated_renamed, index = Date, key = Department)
# Remove 3 outlier specific rows corresponding to the section and dates
section_to_remove <- "Dermato-Oncology"
dates_to_remove <- as.Date(c("2022-09-09", "2022-09-16", "2022-09-23"))


df_tsibble_interpolated_outlier2 <- df_tsibble_interpolated  %>%
  filter(!(Department == section_to_remove & Date %in% dates_to_remove))


# Generate the missing combinations of Sectie and Date
df_missing_dates <- df_tsibble_interpolated_outlier2 %>%
  complete(Department, Date = seq(min(df_tsibble_interpolated_outlier2$Date),
                               max(df_tsibble_interpolated_outlier2$Date),
                               by = "1 week"))

# Fill in missing values

df_filled <- df_missing_dates %>%
  group_by(Department) %>%
  mutate(Zile = zoo::na.approx(Zile, na.rm = FALSE)) %>%
  ungroup()

df_filled2 <- df_filled %>% mutate(Zile = round(Zile, 0))

df_tsibble_interpolated <- as_tsibble(df_filled2,  index = Date, key = Department)
df_tsibble_interpolated$Zile <- abs(df_tsibble_interpolated$Zile)


#remove outlier sections/ , starts with tumori cap si gat

df_tsibble_interpolated_outlier <- df_tsibble_interpolated %>% 
  filter(!grepl("^Head and", Department))


df_tsibble_final <- as_tsibble(df_tsibble_interpolated_outlier,  index = Date, key = Department)


#calculate overall weekly

mean_weekly<-period.apply(df_tsibble_final, INDEX = endpoints(df_tsibble_final, on = "weeks"), FUN = mean)

### or like this
# aggregated <- df_tsibble_final |>
#     aggregate_key(Sectie, Zile = mean(Zile))
#remove sectie empty  column

mean_weekly<- select(mean_weekly, -Department)

#create data frame mean_weekly
mean_weekly_df <- as.data.frame(mean_weekly)
#create date column
mean_weekly_df <- mean_weekly_df %>%
  mutate(Date = rownames(mean_weekly_df), ID = 1:nrow(mean_weekly_df))
rownames(mean_weekly_df) <- NULL
#Mutate date column to date
mean_weekly_df <- mean_weekly_df %>%
mutate(Date = as.Date(Date))

#create tsibble
mean_weekly_df2 <- mean_weekly_df %>% select(-ID) #KEEP IF NEEDED FOR SOMETHING ELSE
mean_weekly_xts <- as.xts(mean_weekly_df2)
mean_weekly_tsibble <- as_tsibble(mean_weekly_df2, index = Date)




#create time series objects 
# 52 weeks in a year. So, start at 2022, week 9
mean_weekly_ts <- ts(mean_weekly_xts, frequency = 52, start = c(2022, 9))
df_ts_interpolated <- ts(df_tsibble_interpolated, frequency = 52, start = c(2022, 9))
df_ts_final <- ts(df_tsibble_final, frequency = 52, start = c(2022, 9))

####take just one year (dec 2022 - to now)
mean_weekly_tsibble_1year <- filter_index(mean_weekly_tsibble, "2022-11-11" ~ .)
df_tsibble_interpolated_1year <- filter_index(df_tsibble_interpolated, "2022-11-11" ~ .)
df_tsibble_final_1year <- filter_index(df_tsibble_final, "2022-11-11" ~ .)

#time series objects 1 year
mean_weekly_tsibble_1year_xts <- as.xts(mean_weekly_tsibble_1year)
mean_weekly_ts_1year <- ts(mean_weekly_tsibble_1year_xts, frequency = 52, start = c(2022, 45))

df_tsibble_interpolated_1year_xts <- as.xts(df_tsibble_interpolated_1year)
df_ts_interpolated_1year <- ts(df_tsibble_interpolated_1year_xts, frequency = 52, start = c(2022, 45))
df_tsibble_final_1year_xts <- as.xts(df_tsibble_final_1year)
df_ts_final_1year <- ts(df_tsibble_final_1year_xts, frequency = 52, start = c(2022, 45))

```
### Exploratory analysis
```{r 3, echo=FALSE}

### mean summary (no outlier section)
summary_mean <- summary(mean_weekly_tsibble)

### summary by sectie (all, with fixed aml)
summary_all <- as.data.frame(df_tsibble_interpolated) %>% 
  select(-Date) %>%
  group_by(Department) %>% 
  summarise(
    mean = round(mean(Zile, na.rm = TRUE), 2), 
    sd = round(sd(Zile, na.rm = TRUE), 2), 
    q1 = quantile(Zile, probs = 0.25, na.rm = TRUE),
    median = median(Zile, na.rm = TRUE), 
    q3 = quantile(Zile, probs = 0.75, na.rm = TRUE),
        iqr = IQR(Zile, na.rm = TRUE),  # Add this line for IQR calculation
    min = min(Zile, na.rm = TRUE), 
    max = max(Zile, na.rm = TRUE), 
    n = n(), 
    n_missing = sum(is.na(Zile)),
    ci_lower = round(mean - qt(0.975, df = n - 1) * (sd / sqrt(n)), 2),
    ci_upper = round(mean + qt(0.975, df = n - 1) * (sd / sqrt(n)), 2)
  ) %>% 
  arrange(mean)


#### Visualise all departments (monthly for clarity)
#
# calculate mean for each month for each sectie using lists 
# #split_data by sectie - list
df_tsibble_interpolated_xts <- as.xts(df_tsibble_interpolated)
split_data <- split(df_tsibble_interpolated_xts, df_tsibble_interpolated_xts$Department)
#calculate mean for each sectie for each month - list
monthly_means <- lapply(split_data, function(x) period.apply(x$Zile, INDEX = endpoints(x, on = "months"), FUN = mean))
#transfomr into df
monthly_means_df <- as.data.frame(monthly_means)
monthly_means_df <- monthly_means_df %>%
  mutate(Date = row.names(monthly_means_df)) %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d"))
#pivot longer
monthly_means_df <- pivot_longer(monthly_means_df, cols = -Date, names_to = "Department", values_to = "Zile")
monthly_means_df <- monthly_means_df %>%
  mutate(Zile = round(Zile, 1))
##graph
graph_ggplot_all<- monthly_means_df %>% 
  ggplot(aes(x = Date, y = Zile, color = Department)) +
   geom_line(aes(group = Department), linetype=1, linewidth=.2) +
  geom_point(size = 0.7) +
  theme_tq() +
  scale_y_continuous(labels = scales::number_format(scale = 1, accuracy = 1)) +
  scale_x_date(date_breaks = "3 months")+
  labs(title = 'Weekly hospital admission delay',
           x = "",
           y = 'Nr. of days')  

graph_all <- ggplotly(graph_ggplot_all)

###outliers determined (3 deps, exclude further)

##### Check for correlations between departments

wide_df <- spread(df_tsibble_final, key = Department, value = Zile)

# Extract numeric columns from wide_df (excluding the Date column)
numeric_columns <- wide_df[, sapply(wide_df, is.numeric)]
numeric_columns <- numeric_columns[,-16]


# Calculate correlations and p-values
# correlation_results <- corr.test(numeric_columns, use = "complete.obs", adjust = "holm", alpha = 0.05, ci = TRUE)
# 
# # Extract correlation matrix and p-values
# correlation_matrix <- correlation_results$r
# p_values <- correlation_results$p


# Convert the correlation data frame into a long format to plot
# correlation_long <- melt(correlation_matrix)
# #rename value
# correlation_long <- correlation_long %>%
#   dplyr::rename(`Pearson's R` = value)
# ### Plot a heatmap of correlations using ggplot2
# heatmap_cor<- ggplot(correlation_long, aes(Var1, Var2, fill = `Pearson's R`)) +
#   geom_tile() +
#   scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
#   theme_minimal() +
#   labs(title = "", x = "", y = "") +   theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust the angle as needed

#! Check psych::cor to plot
# 
# M = cor(numeric_columns)
# 
# #identifying pvalues
# testRes = cor.mtest(numeric_columns, conf.level = 0.95)

# #creating pairwise p value
# pval = as.data.frame(as.table(testRes$p))
# pval <- pval %>% mutate(p_adj=p.adjust(Freq, method='BH'))


library(ggcorrplot)
corr <- round(cor(numeric_columns), 1)
p.mat <- cor_pmat(numeric_columns)
p.mat_longer <- melt(p.mat)
p.mat_longer <- p.mat_longer %>% mutate(p_adj=p.adjust(value, method='BH'))
p.mat_longer <- p.mat_longer %>% select(-value)
#back to wide
p.mat_wide <- spread(p.mat_longer, key = Var2, value = p_adj)
#var1 to rownames
rownames(p.mat_wide) <- p.mat_wide$Var1
p.mat_wide <- p.mat_wide %>% select(-Var1) # ADJUSTED P-vals
hmap<-ggcorrplot(
  corr,
  p.mat = p.mat_wide,
  hc.order = TRUE,
  type = "lower",
  insig = "blank",
  lab = TRUE,
)
##### Visualize mean

#Rolling mean

  smoothed_data <- zoo::rollmean(mean_weekly_tsibble$Zile, k = 5, fill = NA)
  ma_weekly <- mean_weekly_tsibble %>%
    mutate(`5-MA` = smoothed_data)
ma_weekly <- as_tsibble(ma_weekly, index = Date)
ma_weekly2 <- ma_weekly %>%
  select(-Zile)

ma_weekly2 <- as_tsibble(ma_weekly2, index = Date)


### Graph the data with MA
graph_ggplot1<- ma_weekly %>% 
  ggplot(aes(x = Date, y = Zile)) + 
  geom_line(colour = 'darkblue') +
   geom_line(aes(y = `5-MA`), colour = "darkred", linetype = 2, linewidth = 1)+
  theme_tq() +
  scale_y_continuous(labels = scales::number_format(scale = 1, accuracy = 1)) +
  scale_x_date(date_breaks = "3 months")+
  labs(title = 'Weekly hospital admission delay',
           x = "",
           y = 'Nr. of days')  
graph_mean_weekly <- ggplotly(graph_ggplot1)
#also this
graph_ggplot2 <- plot_time_series(mean_weekly_tsibble, Date, Zile)

### show trend by fitting a linear model
linear_model_weekly<- mean_weekly_tsibble %>%
model (linear = TSLM(Zile ~ trend()))

linear_model_weekly_rep <- report(linear_model_weekly) # p val significant for trend

trend_plot <- ggplotly(
  graph_ggplot1 +
  autolayer(fitted(linear_model_weekly_rep), colour ="red") 
)


###ts_decompose()  - "temporal series has less than 2 periods" the time series doesn't have enough observations to capture a seasonal pattern. I have chosen 1 year, so it is normal

####seasonal plot - cycilical but not seasonal. maybe not enough data
#
# seasonality for each dep, wierdly some seem seasonal?
# gg_season(df_tsibble_final) +
#     facet_wrap(vars(Sectie), nrow = 10, scales = "free_y")

#ggplot object with gg_season
graph_seasonal <- #ggplotly(
  gg_season(mean_weekly_tsibble,  pal = c('darkblue', 'darkred'), linewidth = 0.9) + geom_point() +
    theme_tq() +   scale_y_continuous(limits = c(5, 20), labels = scales::number_format(scale = 1, accuracy = 1))+
    labs(title = 'Delay dynamics by year', x = "", y = 'Nr. of days') + scale_x_date(date_breaks = "1 month", date_labels = "%m")
 # )

#same graph but for rolling mean
graph_seasonal_rolling_mean <- #ggplotly(
  gg_season(ma_weekly2,  pal = c('darkblue', 'darkred'), linewidth = 0.9) +
    theme_tq() + geom_point() + scale_y_continuous(limits = c(9, 16), labels = scales::number_format(scale = 1, accuracy = 1)) +
    labs(title = 'Delay dynamics by year', x = "", y = 'Rolling mean') + scale_x_date(date_breaks = "1 month", date_labels = "%m")
  #)

# with ts_seasonal
#
# graph_seasonal <- ts_seasonal(mean_weekly_ts, type = "normal", title = "Delay dynamics by year", palette_normal = "Spectral", Xgrid = FALSE, Ygrid = FALSE)

###acf plot, acf drops quickly, not sure how useful, in non-stationary data it drops slowly
graph_acf <- ts_cor(mean_weekly_ts_1year, lag.max = 52)
graph_acf2 <- ts_cor(mean_weekly_ts, lag.max = 52, seasonal = FALSE)

### STL decomposition

#decompose - does not seem to be seasons
stl_decomp<- df_tsibble_final |>
group_by(Department) %>%
    model(
    STL(Zile ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) |>
  components() 

#stl features
stl_features <- features(df_tsibble_final, Zile, feat_stl)

trend_strength <- ggplot(stl_features, aes(x = Department, y =trend_strength)) +
  geom_col() +
  coord_flip() +
  labs(title = "Trend strength for each section")
```
### Exploratory for Gynecology
```{r} 
### for the purpose of this paper ginecologie will be used as an example because of high variance and unpredictability
dfdf2 <- as.data.frame(subset(df_tsibble_final, Department == "Gynecology")) %>%
  select(-Department)
dfdf2_tsibble <- as_tsibble(dfdf2, index = Date)
dfdf2_xts <- as.xts(dfdf2_tsibble, index = Date)
dfdf2_ts <- ts(dfdf2_xts, frequency = 52, start = c(2022, 9))

unitroot_kpss(dfdf2_tsibble$Zile)
hurst_features <- coef_hurst(dfdf2_tsibble$Zile)
### graph gineco

graph_gineco <- plot_time_series(dfdf2_tsibble, Date, Zile)

graph_ginecologie <- dfdf2 %>% 
  ggplot(aes(x = Date, y = Zile)) + 
  geom_line(colour = 'darkblue') +
  geom_smooth( se = FALSE, colour = "darkred", linetype = 2, span = 0.2)+
  theme_tq() +
  scale_y_continuous(labels = scales::number_format(scale = 1, accuracy = 1)) +
  scale_x_date(date_breaks = "3 months")+
  labs(title = '',
           x = "",
           y = 'Nr. of days')  +   theme(axis.text.x = element_text(angle = 30, hjust = 1))
graph_ginecologie_plotly <- ggplotly(graph_ginecologie)

graph_mean_weekly <- ggplotly(graph_ggplot1)


#Rolling mean gineco

  smoothed_data_gin <- zoo::rollmean(dfdf2_tsibble$Zile, k = 3, fill = NA, align = "right")
  ma_weekly_gin <- dfdf2_tsibble %>%
    mutate(MA = smoothed_data_gin)
ma_weekly_gin <- as_tsibble(ma_weekly_gin, index = Date)
ma_weekly_gin2 <- ma_weekly_gin %>%
  select(-Zile)

ma_weekly_gin2 <- as_tsibble(ma_weekly_gin2, index = Date)

### seasonality gineco

graph_seasonal_gineco <- ts_seasonal(dfdf2_tsibble, type = "normal", title = "", palette_normal = "Spectral", Xgrid = FALSE, Ygrid = FALSE)
graph_seasonal_gineco_ma <- ts_seasonal(ma_weekly_gin2, type = "normal", title = "", palette_normal = "Spectral", Xgrid = FALSE, Ygrid = FALSE)

### acf gineco
graph_acf_gineco <- ts_cor(dfdf2_ts, lag.max = 52, seasonal = FALSE)

### STL decomposition

#decompose - does not seem to be seasons
stl_decomp_gineco<- dfdf2_tsibble |>
    model(
    STL(Zile ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) |>
  components() 

#stl features
stl_features_gineco <- features(dfdf2_tsibble, Zile, feat_stl)


### show trend by fitting a linear model
linear_model_gineco<- dfdf2_tsibble %>%
model (linear = TSLM(Zile ~ trend()))

linear_model_gineco_rep <- report(linear_model_gineco) # p val significant for trend

tslaggs <-  ts_lags(dfdf2_ts, lags = c(1, 2, 3, 26, 39, 52)) #linear correlation between series and lag for 1 and 2 but not for 13, 26, 39, 52 (+1 week to 4 for every 3 mnths)
acf_features <- features(dfdf2_tsibble, Zile, feat_acf, )
acf_gin <- ACF(dfdf2_tsibble, Zile, lag_max = 52)

##### Stationarity and differencing
###check for stationarity
stationary <- df_tsibble_final |>
     group_by(Department) %>%
     features(Zile, unitroot_kpss)

### some departments are not stationary, so we will use the diff() function to make them stationary
stationary_diff_gineco <- dfdf2_tsibble |>
    mutate(diff = difference(Zile)) %>%
    features(diff, unitroot_kpss) 
      #Now stationary

##### Visualise diff 
#calculate diff
df_diff_all <- df_tsibble_final |>
    group_by(Department) %>%
    mutate(diff = difference(Zile))
## calculate mean for each month for each sectie using lists
# split_data by sectie - list
df_diff_all_xts <- as.xts(df_diff_all)
split_data_diff <- split(df_diff_all_xts, df_diff_all_xts$Department)
#calculate mean for each sectie for each month - list
monthly_means_diff <- lapply(split_data_diff, function(x) period.apply(x$diff, INDEX = endpoints(x, on = "months"), FUN = mean))
#transfomr into df
monthly_means_df_diff <- as.data.frame(monthly_means_diff)
monthly_means_df_diff <- monthly_means_df_diff %>%
  mutate(Date = row.names(monthly_means_df_diff)) %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d"))
#pivot longer
monthly_means_df_diff <- pivot_longer(monthly_means_df_diff, cols = -Date, names_to = "Department", values_to = "Difference")
monthly_means_df_diff <- monthly_means_df_diff %>%
  mutate(Difference = round(Difference, 1))

df_diff <- df_diff_all %>%
  subset(Department == "Gynecology") %>%
  select(Date, diff)
df_diff <- as.data.frame(df_diff) %>%
  select(-Department)
df_diff <- as_tsibble(df_diff, index = Date)
### graph
dfdf2_diff <- dfdf2_tsibble |>
    mutate(Difference = difference(Zile))
graph_ggplot_diff_gineco<- dfdf2_diff %>%
  ggplot(aes(x = Date, y = Difference)) +
  geom_line() +
  geom_point(size = 0.7) +
  theme_tq() +
  scale_y_continuous(labels = scales::number_format(scale = 1, accuracy = 1)) +
  scale_x_date(date_breaks = "3 months")+
  labs(title = 'Differenced data',
           x = "",
           y = 'Difference')

graph_gineco_diff <- ggplotly(graph_ggplot_diff_gineco)
####


####  Classical models

#### Split into training and test sets
#
#

#### Below you find a visual representation of the strategy.

# Create ggplot object
visual_rep <- ggplot(dfdf2, aes(Date, Zile)) +
  geom_line(colour = 'darkblue') +
  geom_point(colour = 'black', size = 0.8) +
  geom_smooth(se = FALSE, colour = 'darkred', linetype = "dashed") +
  theme_minimal() +
  scale_y_continuous(labels = scales::number_format(scale = 1, accuracy = 1)) +
  labs(title    = 'Periods for training and testing',
       subtitle = '',
       x = "Date",
       y = 'Nr. of days') +
  annotate(x = ymd('2023-01-04'), y = 55, fill = 'black',
           'text', label = 'Train\nRegion', size = 3.5) +
  annotate(x = ymd('2023-11-05'), y = 55,
           'text', label = 'Test\nRegion', size = 3.5) +
  geom_vline(xintercept = as.numeric(ymd('2023-09-22')), linetype = 'dashed', color = 'cornflowerblue') +
  geom_vline(xintercept = as.numeric(ymd('2023-12-18')), linetype = 'dashed', color = 'cornflowerblue')
# Convert ggplot to plotly
visual_rep_plotly <- ggplotly(visual_rep)

###


#splitting
  train_tbl <- df_tsibble_final %>% filter(Date < ymd("2023-09-22"))
  test_tbl  <- df_tsibble_final %>% filter(Date >= ymd("2023-09-22"))



```
### ML models, method one

```{r}
#   Model         R_squared   MAE  RMSE
#   <chr>             <dbl> <dbl> <dbl>
# 1 AutoML            0.979 0.491 0.646
# 2 GBM               0.975 0.621 0.702
# 3 Random Forest     0.909 1.07  1.33 
# 4 GLM               0.602 2.26  2.78 

#https://dotdata.com/blog/practical-guide-for-feature-engineering-of-time-series-data/
#https://www.analyticsvidhya.com/blog/2019/12/6-powerful-feature-engineering-techniques-time-series/#h-setting-up-the-problem-statement-for-time-series-data


###Feature creation and preprocessing
#no trend apparently if you include rollmean . k= 3 yields the highest rsq value when compared to k = 5, 7, 9, 11, 13

# Fit the linear model
linear_model_gineco_ML <- dfdf2_tsibble %>%
  model(linear = TSLM(Zile ~ trend()))

# Extract the linear trend component
linear_trend <- as.data.frame(fitted(linear_model_gineco_ML)) %>%
  select(.fitted)

linear_trend <- as.numeric(linear_trend$.fitted)

#exp mean
Expanding_means <- rollapply(dfdf2_ts, width = seq_along(dfdf2_ts), FUN = mean, align = "right")
Expanding_means <- as.numeric(Expanding_means)

#### Sample holidays (excluding the year) - Check if holiday was within a week
holidays <- c("01-01", "07-01", "08-03", "04-09", "05-01", "09-05", "27-08", "31-08", "06-10", "25-12")
# Set a time window (e.g., within a week)
time_window <- 7
# Function to calculate binary indicator for adjacent holidays
is_adjacent_holiday <- function(data_date) {
  adjacent <- any(abs(as.Date(holidays, format = "%d-%m") - as.Date(data_date)) <= time_window)
  return(as.numeric(adjacent))
}

# Create binary indicator variable
adj_holid <- dfdf2 %>%
  mutate(adjacent_holiday = sapply(Date, is_adjacent_holiday))
adj_holid <- adj_holid %>%
  select(adjacent_holiday)
adj_holid <- as.numeric(adj_holid$adjacent_holiday)




#not enough data to capture long term trends I think
model_data_tbl <- dfdf2_tsibble %>% 
  mutate( Linear_trend = linear_trend,
          # Rolling_mean_2 = rollmean(Zile, k = 2, fill = NA, align = "right"),
          # Rolling_mean_5 = rollmean(Zile, k = 5, fill = NA, align = "right"),
           Rolling_mean_7 = rollmean(Zile, k = 7, fill = NA, align = "right"),
          Rolling_mean_9 = rollmean(Zile, k = 9, fill = NA, align = "right"),
          Rolling_mean_13 = rollmean(Zile, k = 13, fill = NA, align = "right"),
          # Rolling_mean_26 = rollmean(Zile, k = 26, fill = NA, align = "right"),
          # SD_2 = rollapply(Zile, width = 2, FUN = sd, fill = NA, align = "right"),
          # SD_5 = rollapply(Zile, width = 5, FUN = sd, fill = NA, align = "right"),
           SD_7 = rollapply(Zile, width = 7, FUN = sd, fill = NA, align = "right"),
          SD_9 = rollapply(Zile, width = 9, FUN = sd, fill = NA, align = "right"),
          SD_13 = rollapply(Zile, width = 13, FUN = sd, fill = NA, align = "right"),
          # SD_26 = rollapply(Zile, width = 26, FUN = sd, fill = NA, align = "right"),
          Expanding_mean = Expanding_means,
          Rev_lag_1   = lag(Zile, n = 1),
          Rev_lag_2   = lag(Zile, n = 2),
          Rev_lag_3   = lag(Zile, n = 3),
          Rev_lag_4  = lag(Zile, n = 4),
          Rev_lag_5   = lag(Zile, n = 5),
          Rev_lag_6   = lag(Zile, n = 6),
          Rev_lag_7   = lag(Zile, n = 7),
    # Rolling_mean_2_Lag_1 = Rolling_mean_2 * Rev_lag_1,
    # Rolling_mean_2_Lag_2 = Rolling_mean_2 * Rev_lag_2,
    # Rolling_mean_13_Lag_1 = Rolling_mean_13 * Rev_lag_1,
    # Rolling_mean_13_Lag_2 = Rolling_mean_13 * Rev_lag_2,
    # Rolling_mean_26_Lag_1 = Rolling_mean_26 * Rev_lag_1,
    # Rolling_mean_26_Lag_2 = Rolling_mean_26 * Rev_lag_2,
    # EMA_2 = EMA(Zile, n = 2),
    # EMA_5 = EMA(Zile, n = 5),
     EMA_7 = EMA(Zile, n = 7),
    EMA_9 = EMA(Zile, n = 9),
    EMA_13 = EMA(Zile, n = 13),
    # DEMA_2 = DEMA(Zile, n = 2),
    # DEMA_5 = DEMA(Zile, n = 5),
     DEMA_7 = DEMA(Zile, n = 7),
    DEMA_9 = DEMA(Zile, n = 9),
    DEMA_13 = DEMA(Zile, n = 13),
    # DEMA_26 = DEMA(Zile, n = 26),
    # WMA_2 = WMA(Zile, n = 2),
    # WMA_5 = WMA(Zile, n = 5),
     WMA_7 = WMA(Zile, n = 7),
    WMA_9 = WMA(Zile, n = 9),
    WMA_13 = WMA(Zile, n = 13),
    # EMA_2_Lag_1 = EMA_2 * Rev_lag_1,
    # EMA_2_Lag_2 = EMA_2 * Rev_lag_2,
    # EMA_13_Lag_1 = EMA_13 * Rev_lag_1,
    # EMA_13_Lag_2 = EMA_13 * Rev_lag_2,
    # EMA_5_Lag_1 = EMA_5 * Rev_lag_1,
    # EMA_5_Lag_2 = EMA_5 * Rev_lag_2,
    # EMA_7_Lag_1 = EMA_7 * Rev_lag_1,
    # EMA_7_Lag_2 = EMA_7 * Rev_lag_2,
    # EMA_9_Lag_1 = EMA_9 * Rev_lag_1,
    # EMA_9_Lag_2 = EMA_9 * Rev_lag_2,
    #       month_january = as.numeric(lubridate::month(Date) == 1),
    # month_february = as.numeric(lubridate::month(Date) == 2),
    # month_march = as.numeric(lubridate::month(Date) == 3),
    # month_april = as.numeric(lubridate::month(Date) == 4),
    # month_may = as.numeric(lubridate::month(Date) == 5),
    # month_june = as.numeric(lubridate::month(Date) == 6),
    # month_july = as.numeric(lubridate::month(Date) == 7),
    # month_august = as.numeric(lubridate::month(Date) == 8),
    # month_september = as.numeric(lubridate::month(Date) == 9),
    # month_october = as.numeric(lubridate::month(Date) == 10),
    # month_november = as.numeric(lubridate::month(Date) == 11),
    # month_december = as.numeric(lubridate::month(Date) == 12),
    Month = lubridate::month(Date),
    Quarter = lubridate::quarter(Date),
     Year = lubridate::year(Date),
    Week_of_Year = isoweek(Date),
    Day_of_year = yday(Date),
    Outlier_indicator = as.numeric(Zile < quantile(Zile, 0.05) | Zile > quantile(Zile, 0.95)),
     Adjacent_holiday = adj_holid
        ) %>% filter(!is.na(Rev_lag_1) & !is.na(Rev_lag_7)  & !is.na(SD_13)
           # !is.na(Rolling_mean_2) & !is.na(Rolling_mean_13)  & 
           )

#moving window statistics. This involves aggregating the time series values over a rolling window. By doing so, noise is smoothed out, shifting the focus to the underlying trends. Moving windows can help identify patterns that may not be immediately apparent in the raw data. 
#The lag value we choose will depend on the correlation of individual values with its past values.
# so acf and pacf plots to determine lag values. lag 1 and 2 are strongest
```


###train 1
```{r}
#Split into train test and FC

train_tbl_ML2 <- model_data_tbl %>% filter(Date < ymd("2022-12-02"))
test_tbl_ML2  <- model_data_tbl %>% filter(Date >= ymd("2022-12-02") & Date <= ymd("2023-01-20"))


### Modelling
  
  #initiate h2o
  h2o.init(max_mem_size = "16G")
h2o.no_progress()

# response variable
y <- "Zile"

# predictors set: remove response variable and date from the set
x <- setdiff(names(train_tbl_ML2 %>% as.h2o()), c(y, "Date"))

### Random Forrest
rft_model22 <- 
  h2o.automl(
    x = x,
    y = y,
    training_frame = train_tbl_ML2 %>% as.h2o(),
    nfolds = 0,
    validation_frame = test_tbl_ML2 %>% as.h2o(),
    include_algos = c("DRF"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
  rft_model2 <- rft_model22@leader
# # RFT hyperparameters unfinished!
# rft_params1 <- list(mtries = c(3,6,9),
#   learn_rate = c(0.01, 0.1),
#                     max_depth = c(1, 3, 5, 9),
#                     sample_rate = c(0.4,0.632, 0.8),
#                     col_sample_rate = c(0.2, 0.5, 1.0))


# gradient boosting machine model
gbm_model22 <-  
 h2o.automl(
    x = x,
    y = y,
    training_frame = train_tbl_ML2 %>% as.h2o(),
    nfolds = 0,
    validation_frame = test_tbl_ML2 %>% as.h2o(),
    include_algos = c("GBM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 gbm_model2 <- gbm_model22@leader

# # GBM hyperparameters (bigger grid than above)
# # GBM hyperparameters
# gbm_params1 <- list(learn_rate = c(0.01, 0.1),
#                     max_depth = c(1, 3, 5, 9),
#                     sample_rate = c(0.8, 1.0),
#                     col_sample_rate = c(0.2, 0.5, 1.0))
# 
# search_criteria <- list(strategy = "RandomDiscrete", stopping_metric = "AUTO", stopping_rounds = 10, stopping_tolerance = 0.005)
# 
# # Train and validate a cartesian grid of GBMs
# gbm_grid1 <- h2o.grid("gbm", x = x, y = y,
#                       grid_id = "gbm_grid1",
#                       training_frame = train_stretch %>% as.h2o(),
#                         fold_column = ".id",
#                       ntrees = 500,
#                           seed = 1975,
#                                   hyper_params = gbm_params1)
# 
# # Get the grid results, sorted by validation AUC
# gbm_gridperf1 <- h2o.getGrid(grid_id = "gbm_grid1",
#                              sort_by = "RMSE",
#                              decreasing = FALSE)
# best_gbm1 <- h2o.getModel(gbm_gridperf1@model_ids[[1]])
# 
# best_gbm_perf1 <- h2o.performance(model = best_gbm1,
#                                   newdata = as.h2o(test))

# generalised linear model 
glm_model22 <- 
 h2o.automl(
    x = x,
    y = y,
    training_frame = train_tbl_ML2 %>% as.h2o(),
    nfolds = 0,
    validation_frame = test_tbl_ML2 %>% as.h2o(),
    include_algos = c("GLM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
  glm_model2 <- glm_model22@leader

#Deep learning
automl_model2 <-
  h2o.automl(
    x = x,
    y = y,
    training_frame = train_tbl_ML2 %>% as.h2o(),
    nfolds = 0,
    validation_frame = test_tbl_ML2 %>% as.h2o(),
    include_algos = c("DeepLearning"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
    # see best auto model
      automl_model2@leaderboard
      
### Extract and save the leader autoML model
  aml_model2 <- automl_model2@leader
  
    #performances
perf_rft2 <- h2o.performance(rft_model2, newdata = as.h2o(test_tbl_ML2))
perf_gbm2 <- h2o.performance(gbm_model2, newdata = as.h2o(test_tbl_ML2))
perf_glm2 <- h2o.performance(glm_model2, newdata = as.h2o(test_tbl_ML2))
perf_aml2 <- h2o.performance(aml_model2, newdata = as.h2o(test_tbl_ML2))

metrics_table2 <- tibble(
  Model = c("Random Forest 1", "GBM 1", "GLM 1", "DL 1"),
  R_squared = c(
    h2o.r2(perf_rft2),
    h2o.r2(perf_gbm2),
    h2o.r2(perf_glm2),
    h2o.r2(perf_aml2)
  ),
  MAE = c(
    h2o.mae(perf_rft2),
    h2o.mae(perf_gbm2),
    h2o.mae(perf_glm2),
    h2o.mae(perf_aml2)
  ),
  RMSE = c(
    h2o.rmse(perf_rft2),
    h2o.rmse(perf_gbm2),
    h2o.rmse(perf_glm2),
    h2o.rmse(perf_aml2)
  )
) %>% arrange(desc(R_squared))
```
### train 2
```{r}
#Split into train test and FC

train_tbl_ML3 <- model_data_tbl %>% filter(Date < ymd("2023-01-27"))
  test_tbl_ML3  <- model_data_tbl %>% filter(Date >= ymd("2023-01-27") & Date <= ymd("2023-03-17"))
  
  
  ### Modelling

# response variable
y <- "Zile"

# predictors set: remove response variable and date from the set
x <- setdiff(names(train_tbl_ML3 %>% as.h2o()), c(y, "Date"))

### Random Forrest
rft_model33 <- 
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML3),
    nfolds             = 0,
     validation_frame   = test_tbl_ML3 %>% as.h2o(),
        include_algos = c("DRF"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 rft_model3 <- rft_model33@leader
# gradient boosting machine model
gbm_model33 <-  
 h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML3),
    nfolds             = 0,
     validation_frame   = test_tbl_ML3 %>% as.h2o(),
        include_algos = c("GBM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
  gbm_model3 <- gbm_model33@leader


# generalised linear model 
glm_model33 <- 
 h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML3),
    nfolds             = 0,
     validation_frame   = test_tbl_ML3 %>% as.h2o(),
        include_algos = c("GLM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 glm_model3 <- glm_model33@leader

#automl funciton
automl_model3 <-
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML3),
    nfolds             = 0,
     validation_frame   = test_tbl_ML3 %>% as.h2o(),
        include_algos = c("DeepLearning"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 
      
### Extract and save the leader autoML model
  aml_model3 <- automl_model3@leader
  
    #performances
perf_rft3 <- h2o.performance(rft_model3, newdata = test_tbl_ML3 %>% as.h2o())
perf_gbm3 <- h2o.performance(gbm_model3, newdata = as.h2o(test_tbl_ML3))
perf_glm3 <- h2o.performance(glm_model3, newdata = as.h2o(test_tbl_ML3))
perf_aml3 <- h2o.performance(aml_model3, newdata = as.h2o(test_tbl_ML3))

metrics_table3 <- tibble(
  Model = c("Random Forest 2", "GBM 2", "GLM 2", "DL 2"),
  R_squared = c(
    h2o.r2(perf_rft3),
    h2o.r2(perf_gbm3),
    h2o.r2(perf_glm3),
    h2o.r2(perf_aml3)
  ),
  MAE = c(
    h2o.mae(perf_rft3),
    h2o.mae(perf_gbm3),
    h2o.mae(perf_glm3),
    h2o.mae(perf_aml3)
  ),
  RMSE = c(
    h2o.rmse(perf_rft3),
    h2o.rmse(perf_gbm3),
    h2o.rmse(perf_glm3),
    h2o.rmse(perf_aml3)
  )
) %>% arrange(desc(R_squared))

```
### train 3
```{r}
#Split into train test and FC

train_tbl_ML4 <- model_data_tbl %>% filter(Date < ymd("2023-03-24"))
  test_tbl_ML4  <- model_data_tbl %>% filter(Date >= ymd("2023-03-24") & Date <= ymd("2023-05-12"))
  
  
  ### Modelling

# response variable
y <- "Zile"

# predictors set: remove response variable and date from the set
x <- setdiff(names(train_tbl_ML4 %>% as.h2o()), c(y, "Date"))

### Random Forrest
rft_model44 <- 
 h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML4),
    nfolds             = 0,
     validation_frame   = test_tbl_ML4 %>% as.h2o(),
            include_algos = c("DRF"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 rft_model4<- rft_model44@leader

# gradient boosting machine model
gbm_model44 <-  
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML4),
    nfolds             = 0,
     validation_frame   = test_tbl_ML4 %>% as.h2o(),
            include_algos = c("GBM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
gbm_model4 <- gbm_model44@leader
# generalised linear model 
glm_model44 <- 
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML4),
    nfolds             = 0,
     validation_frame   = test_tbl_ML4 %>% as.h2o(),
            include_algos = c("GLM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
glm_model4 <- glm_model44@leader
#automl funciton
automl_model4 <-
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML4),
    nfolds             = 0,
     validation_frame   = test_tbl_ML4 %>% as.h2o(),
            include_algos = c("DeepLearning"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 
      
### Extract and save the leader autoML model
  aml_model4 <- automl_model4@leader
  
    #performances
perf_rft4 <- h2o.performance(rft_model4, newdata = test_tbl_ML4 %>% as.h2o())
perf_gbm4 <- h2o.performance(gbm_model4, newdata = as.h2o(test_tbl_ML4))
perf_glm4 <- h2o.performance(glm_model4, newdata = as.h2o(test_tbl_ML4))
perf_aml4 <- h2o.performance(aml_model4, newdata = as.h2o(test_tbl_ML4))

metrics_table4 <- tibble(
  Model = c("Random Forest 3", "GBM 3", "GLM 3", "DL 3"),
  R_squared = c(
    h2o.r2(perf_rft4),
    h2o.r2(perf_gbm4),
    h2o.r2(perf_glm4),
    h2o.r2(perf_aml4)
  ),
  MAE = c(
    h2o.mae(perf_rft4),
    h2o.mae(perf_gbm4),
    h2o.mae(perf_glm4),
    h2o.mae(perf_aml4)
  ),
  RMSE = c(
    h2o.rmse(perf_rft4),
    h2o.rmse(perf_gbm4),
    h2o.rmse(perf_glm4),
    h2o.rmse(perf_aml4)
  )
) %>% arrange(desc(R_squared))
```
### train 4
```{r}
#Split into train test and FC

train_tbl_ML5 <- model_data_tbl %>% filter(Date < ymd("2023-05-19"))
  test_tbl_ML5  <- model_data_tbl %>% filter(Date >= ymd("2023-05-19") & Date <= ymd("2023-07-07"))
  
 

  ### Modelling

# response variable
y <- "Zile"

# predictors set: remove response variable and date from the set
x <- setdiff(names(train_tbl_ML5 %>% as.h2o()), c(y, "Date"))


# dl <- h2o.deeplearning(x = x,
#                        y = y,
#                        hidden = c(1),
#                        epochs = 1000,
#                        train_samples_per_iteration = -1,
#                        reproducible = TRUE,
#                        activation = "Tanh",
#                        single_node_mode = FALSE,
#                        balance_classes = FALSE,
#                        force_load_balance = FALSE,
#                        seed = 23123,
#                        tweedie_power = 1.5,
#                        score_training_samples = 0,
#                        score_validation_samples = 0,
#                        training_frame = insurance,
#                        stopping_rounds = 0)


### Random Forrest
rft_model55 <- 
   h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML5),
    nfolds             = 0,
     validation_frame   = test_tbl_ML5 %>% as.h2o(),
                include_algos = c("DRF"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 rft_model5 <- rft_model55@leader

# gradient boosting machine model
gbm_model55 <-  
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML5),
    nfolds             = 0,
     validation_frame   = test_tbl_ML5 %>% as.h2o(),
                include_algos = c("GBM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
gbm_model5 <- gbm_model55@leader
# generalised linear model 
glm_model55 <- 
 h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML5),
    nfolds             = 0,
     validation_frame   = test_tbl_ML5 %>% as.h2o(),
                include_algos = c("GLM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
glm_model5 <- glm_model55@leader
#automl funciton
automl_model5 <-
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML5),
    nfolds             = 0,
     validation_frame   = test_tbl_ML5 %>% as.h2o(),
                include_algos = c("DeepLearning"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
 
      
### Extract and save the leader autoML model
  aml_model5 <- automl_model5@leader
  
    #performances
perf_rft5 <- h2o.performance(rft_model5, newdata = test_tbl_ML5 %>% as.h2o())
perf_gbm5 <- h2o.performance(gbm_model5, newdata = as.h2o(test_tbl_ML5))
perf_glm5 <- h2o.performance(glm_model5, newdata = as.h2o(test_tbl_ML5))
perf_aml5 <- h2o.performance(aml_model5, newdata = as.h2o(test_tbl_ML5))

metrics_table5 <- tibble(
  Model = c("Random Forest 4", "GBM 4", "GLM 4", "DL 4"),
  R_squared = c(
    h2o.r2(perf_rft5),
    h2o.r2(perf_gbm5),
    h2o.r2(perf_glm5),
    h2o.r2(perf_aml5)
  ),
  MAE = c(
    h2o.mae(perf_rft5),
    h2o.mae(perf_gbm5),
    h2o.mae(perf_glm5),
    h2o.mae(perf_aml5)
  ),
  RMSE = c(
    h2o.rmse(perf_rft5),
    h2o.rmse(perf_gbm5),
    h2o.rmse(perf_glm5),
    h2o.rmse(perf_aml5)
  )
) %>% arrange(desc(R_squared))
```
## train 5

``` {r}

#Split into train test and FC

train_tbl_ML <- model_data_tbl %>% filter(Date < ymd("2023-07-14"))
  test_tbl_ML  <- model_data_tbl %>% filter(Date >= ymd("2023-07-14") & Date <= ymd("2023-09-01"))
  
  
  

# response variable
y <- "Zile"

# predictors set: remove response variable and date from the set
x <- setdiff(names(train_tbl_ML %>% as.h2o()), c(y, "Date"))

### Random Forrest
rft_model6 <- 
   h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML),
    nfolds             = 0,
     validation_frame   = test_tbl_ML %>% as.h2o(),
                    include_algos = c("DRF"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
rft_model <- rft_model6@leader
# gradient boosting machine model
gbm_model6 <-  
   h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML),
    nfolds             = 0,
     validation_frame   = test_tbl_ML %>% as.h2o(),
                    include_algos = c("GBM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
gbm_model <- gbm_model6@leader
# generalised linear model 
glm_model6 <- 
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML),
    nfolds             = 0,
     validation_frame   = test_tbl_ML %>% as.h2o(),
                    include_algos = c("GLM"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
glm_model <- glm_model6@leader

#automl funciton
automl_model <-
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train_tbl_ML),
    nfolds             = 0,
     validation_frame   = test_tbl_ML %>% as.h2o(),
                    include_algos = c("DeepLearning"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 60,
    sort_metric       = "RMSE",
    seed               = 1975
 )
    # see best auto model
      automl_model@leaderboard
      
### Extract and save the leader autoML model
  aml_model <- automl_model@leader
  #performances
perf_rft <- h2o.performance(rft_model, newdata = test_tbl_ML %>% as.h2o())
perf_gbm <- h2o.performance(gbm_model, newdata = as.h2o(test_tbl_ML))
perf_glm <- h2o.performance(glm_model, newdata = as.h2o(test_tbl_ML))
perf_aml <- h2o.performance(aml_model, newdata = as.h2o(test_tbl_ML))

metrics_table <- tibble(
  Model = c("Random Forest 5", "GBM 5", "GLM 5", "DL 5"),
  R_squared = c(
    h2o.r2(perf_rft),
    h2o.r2(perf_gbm),
    h2o.r2(perf_glm),
    h2o.r2(perf_aml)
  ),
  MAE = c(
    h2o.mae(perf_rft),
    h2o.mae(perf_gbm),
    h2o.mae(perf_glm),
    h2o.mae(perf_aml)
  ),
  RMSE = c(
    h2o.rmse(perf_rft),
    h2o.rmse(perf_gbm),
    h2o.rmse(perf_glm),
    h2o.rmse(perf_aml)
  )
) %>% arrange(desc(R_squared))
```
###  Model performance
```{r}
    ### Model performance

  #rft_model@model$model_summary

#combine metrics tables
metrics_table_final <- rbind(metrics_table5, metrics_table2, metrics_table3, metrics_table4, metrics_table)

metrics_table_final$Model <- gsub("\\d", "", metrics_table_final$Model)

#calculate mean of metrics for each model
metrics_table_mean <- metrics_table_final %>% group_by(Model) %>% summarise_all(mean)
```
###  Forecast on the Validation data set 
```{r}
# NAMES TEST and VALIDATION ARE SWAPPED HERE
test <- model_data_tbl %>% filter(Date >= ymd("2023-09-08") & Date <= ymd("2023-10-27"))
validation <- model_data_tbl %>% filter(Date > ymd("2023-10-27"))
train <- model_data_tbl %>% filter(Date < ymd("2023-09-08"))


# response variable
y <- "Zile"

# predictors set: remove response variable and date from the set
x <- setdiff(names(train %>% as.h2o()), c(y, "Date"))
#use only the two best models 
DL_test <-
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train),
    nfolds             = 0,
     validation_frame   = test %>% as.h2o(),
                    include_algos = c("DeepLearning"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 120,
    sort_metric       = "RMSE",
    seed               = 1975
 )

# # gradient boosting machine model
# gbm_test <-  
#    h2o.automl(
#     x = x,
#     y = y,
#     training_frame     = as.h2o(train),
#     nfolds             = 0,
#      validation_frame   = test  %>% as.h2o(),
#                     include_algos = c("GBM"),
#     stopping_metric    = "RMSE",
#     stopping_rounds    = 10,
#     stopping_tolerance = 0.005,
#     max_runtime_secs   = 120,
#     sort_metric       = "RMSE",
#     seed               = 1975
#  )

DL_model_test <- DL_test@leader
perf_dl_test <- h2o.performance(DL_model_test, newdata = as.h2o(test))
# gbm_model_test <- gbm_test@leader
# perf_gbm_test <- h2o.performance(gbm_model_test, newdata = as.h2o(test))


metrics_test <- tibble(
  Model = c( "DL "),
  R_squared = c(
    # h2o.r2(perf_gbm_test),
    h2o.r2(perf_dl_test)
  ),
  MAE = c(
  # h2o.mae(perf_gbm_test),
    h2o.mae(perf_dl_test)
  ),
  RMSE = c(
   # h2o.rmse(perf_gbm_test),
    h2o.rmse(perf_dl_test)
  )
) %>% arrange(desc(R_squared))
```

###  Forecast on the TEST,  Actual vs predicted

```{r}

perf_dl_validation <- h2o.performance(DL_model_test, newdata = as.h2o(validation))
metrics_validation <- tibble(
  Model = c(  "DL "),
  R_squared = c(
    
    h2o.r2(perf_dl_validation)
  ),
  MAE = c(
 
    h2o.mae(perf_dl_validation)
  ),
  RMSE = c(
  
    h2o.rmse(perf_dl_validation)
  )
) %>% arrange(desc(R_squared))
### Actual vs Predicted


predictions_dl <- as.vector(h2o.predict(DL_model_test, as.h2o(validation))$predict)


 actual_values_test <- as.data.frame(validation) %>%
  select(Date, Zile)
actual_vs_predicted_test <- cbind(actual_values_test,  DL = predictions_dl) %>% dplyr::rename(Actual = Zile)

plot_final <-actual_vs_predicted_test %>%
 plot_ly() %>% 
    add_lines(x = ~ Date, y = ~ Actual, name = 'Actual values', line = list(color = 'darkblue')) %>% 
    
    add_lines(x = ~ Date, y = ~ DL, name = 'Deep Learning', 
              line = list(dash = 'dot', color = 'red')) %>% 
     layout(
    title = list(text = '', font = list(size = 16, color = 'black')),
    yaxis = list(title = list(text = 'Nr. of days', font = list(size = 14, color = 'black'))),
    xaxis = list(title = list(text = ''), font = list(size = 14)),
    legend = list(orientation = 'h', font = list(size = 12))
  )

```

### VIP and AvP plots (first test), best model
```{r}
  #aml_model %>% h2o.varimp_plot()

  ###Variable importance plots
# p_glm <- vip(glm_model2) + ggtitle("GLM")
# p_rft <- vip(rft_model2) + ggtitle("RF")
# p_gbm <- vip(gbm_model_test) + ggtitle("GBM")
p_aml <- vip(DL_model_test) + ggtitle("DL")

vi_plot <- grid.arrange(p_gbm, p_aml, nrow = 2)

  #### AVP
# predictions_gbm <- as.vector(h2o.predict(gbm_model_test, as.h2o(test))$predict)
predictions_aml <- as.vector(h2o.predict(DL_model_test, as.h2o(test))$predict)
# predictions_rft <- as.vector(h2o.predict(rft_model2, as.h2o(test))$predict)
# predictions_glm <- as.vector(h2o.predict(glm_model2, as.h2o(test))$predict)

 actual_values_validation <- as.data.frame(test) %>%
  select(Date, Zile)
actual_vs_predicted_validation <- cbind(actual_values_validation,    DL = predictions_aml) %>% dplyr::rename(Actual = Zile)


### Plot a vs p TEST
plot_avp <-actual_vs_predicted_validation %>%
 plot_ly() %>% 
    add_lines(x = ~ Date, y = ~ Actual, name = 'Actual values', line = list(color = 'darkblue')) %>% 
     # add_lines(x = ~ Date, y = ~ RFT, name = 'Random Forest', 
     #           line = list(dash = 'dot', color = 'orange')) %>% 
    # add_lines(x = ~ Date, y = ~ GBM, name = 'Gradient Boosting', 
    #           line = list(dash = 'dash', color = 'green')) %>% 
    add_lines(x = ~ Date, y = ~ DL, name = 'Deep Learning', 
              line = list(dash = 'dot', color = 'red')) %>% 
    #  add_lines(x = ~ Date, y = ~ GLM, name = 'Generalised Linear Model', 
    #            line = list(dash = 'dash', color = 'purple')) %>% 
     layout(
    title = list(text = '', font = list(size = 16, color = 'black')),
    yaxis = list(title = list(text = 'Nr. of days', font = list(size = 14, color = 'black'))),
    xaxis = list(title = list(text = ''), font = list(size = 14)),
    legend = list(orientation = 'h', font = list(size = 12))
  )

### Cross-correlation
Actual <- actual_vs_predicted_test$Actual
Predicted <- actual_vs_predicted_test$DL
actual_vs_predicted_validation_tsibble <- actual_vs_predicted_validation %>%
  as_tsibble(index = Date)
crosscor<- ccf(Actual, Predicted, lag.max = 15, plot = TRUE, ylab = "Cross-correlation", main = "")
crosscor2<- CCF(actual_vs_predicted_validation_tsibble, Actual, DL)
# predictions_train <- DL_model_test %>% h2o.predict(as.h2o(train)) %>% as.vector()
# actual_values_train <- as.data.frame(train) %>%
#   select(Date, Zile)
# actual_vs_predicted_train <- cbind(actual_values_train, DL = predictions_train) %>% dplyr::rename(Actual = Zile)
# Actual2 <- actual_vs_predicted_train$Actual
# Predicted2 <- actual_vs_predicted_train$DL
# crosscor2<- ccf(Actual2, Predicted2, lag.max = 30, plot = TRUE, ylab = "Cross-correlation", main = "")

# # Plot the overall time series
# 
# 
# graph_final <- dfdf2 %>% 
#   ggplot(aes(x = Date, y = Zile)) + 
#   geom_line(colour = 'darkblue') +
#   geom_line(data = actual_vs_predicted_validation, aes(x = Date, y = RFT, linetype = 'Random Forest', color = 'Random Forest'), size =0.8) +
#   geom_line(data = actual_vs_predicted_validation, aes(x = Date, y = GBM, linetype = 'Gradient Boosting', color = 'Gradient Boosting')) +
#   geom_line(data = actual_vs_predicted_validation, aes(x = Date, y = GLM, linetype = 'Generalised Linear Model', color = 'Generalised Linear Model')) +
#   geom_line(data = actual_vs_predicted_validation, aes(x = Date, y = AML, linetype = 'Auto ML', color = 'Auto ML'), size = 0.8) +
#   theme_minimal() +
#   scale_y_continuous(labels = scales::number_format(scale = 1, accuracy = 1))  +
#   labs(linetype ='',
#        colour ='') +
#   scale_color_manual(values = c('darkblue', 'Random Forest' = 'orange', 'Gradient Boosting' = 'green', 'Generalised Linear Model' = 'purple', 'Auto ML' = 'red')) +
#   scale_linetype_manual(values = c('solid', 'Random Forest' = 'dot', 'Gradient Boosting Machine' = 'dash', 'Generalised Linear Model' = 'dash', 'Auto ML' = 'dot'))
# 
# # Convert ggplot to plotly
# graph_final_plotly <- ggplotly(graph_final) %>%
#   layout(
#     title = list(text = '', font = list(size = 16, color = 'black')),
#     yaxis = list(title = list(text = 'Nr. of days', font = list(size = 14, color = 'black'))),
#     xaxis = list(title = list(text = ''), font = list(size = 14)),
#     legend = list(orientation = 'h', font = list(size = 12))
#   )




```
###Forecast on differenced data
```{r}



#exp mean.
df_diff <- df_diff [-1,]
df_diff_xts <- as.xts(df_diff)
df_diff_ts <- ts(df_diff_xts, frequency = 52, start = c(2022, 9))
Expanding_means <- rollapply(df_diff_ts, width = seq_along(df_diff_ts), FUN = mean, align = "right")
Expanding_means <- as.numeric(Expanding_means)



# Create binary indicator variable
adj_holid2 <- df_diff %>%
  mutate(adjacent_holiday = sapply(Date, is_adjacent_holiday))
adj_holid2 <- adj_holid2 %>%
  select(adjacent_holiday)
adj_holid2 <- as.numeric(adj_holid2$adjacent_holiday)




#not enough data to capture long term trends I think
model_data_tbl2 <- df_diff %>% 
  mutate( # Rolling_mean_2 = rollmean(Zile, k = 2, fill = NA, align = "right"),
          # Rolling_mean_5 = rollmean(Zile, k = 5, fill = NA, align = "right"),
           Rolling_mean_7 = rollmean(diff, k = 7, fill = NA, align = "right"),
          Rolling_mean_9 = rollmean(diff, k = 9, fill = NA, align = "right"),
          Rolling_mean_13 = rollmean(diff, k = 13, fill = NA, align = "right"),
          # Rolling_mean_26 = rollmean(Zile, k = 26, fill = NA, align = "right"),
          # SD_2 = rollapply(Zile, width = 2, FUN = sd, fill = NA, align = "right"),
          # SD_5 = rollapply(Zile, width = 5, FUN = sd, fill = NA, align = "right"),
           SD_7 = rollapply(diff, width = 7, FUN = sd, fill = NA, align = "right"),
          SD_9 = rollapply(diff, width = 9, FUN = sd, fill = NA, align = "right"),
          SD_13 = rollapply(diff, width = 13, FUN = sd, fill = NA, align = "right"),
          # SD_26 = rollapply(Zile, width = 26, FUN = sd, fill = NA, align = "right"),
          Expanding_mean = Expanding_means,
          Rev_lag_1   = lag(diff, n = 1),
          Rev_lag_2   = lag(diff, n = 2),
          Rev_lag_3   = lag(diff, n = 3),
          Rev_lag_4  = lag(diff, n = 4),
          Rev_lag_5   = lag(diff, n = 5),
          Rev_lag_6   = lag(diff, n = 6),
          Rev_lag_7   = lag(diff, n = 7),
    # Rolling_mean_2_Lag_1 = Rolling_mean_2 * Rev_lag_1,
    # Rolling_mean_2_Lag_2 = Rolling_mean_2 * Rev_lag_2,
    # Rolling_mean_13_Lag_1 = Rolling_mean_13 * Rev_lag_1,
    # Rolling_mean_13_Lag_2 = Rolling_mean_13 * Rev_lag_2,
    # Rolling_mean_26_Lag_1 = Rolling_mean_26 * Rev_lag_1,
    # Rolling_mean_26_Lag_2 = Rolling_mean_26 * Rev_lag_2,
    # EMA_2 = EMA(Zile, n = 2),
    # EMA_5 = EMA(Zile, n = 5),
     EMA_7 = EMA(diff, n = 7),
    EMA_9 = EMA(diff, n = 9),
    EMA_13 = EMA(diff, n = 13),
    # DEMA_2 = DEMA(Zile, n = 2),
    # DEMA_5 = DEMA(Zile, n = 5),
     DEMA_7 = DEMA(diff, n = 7),
    DEMA_9 = DEMA(diff, n = 9),
    DEMA_13 = DEMA(diff, n = 13),
    # DEMA_26 = DEMA(Zile, n = 26),
    # WMA_2 = WMA(Zile, n = 2),
    # WMA_5 = WMA(Zile, n = 5),
     WMA_7 = WMA(diff, n = 7),
    WMA_9 = WMA(diff, n = 9),
    WMA_13 = WMA(diff, n = 13),
    # EMA_2_Lag_1 = EMA_2 * Rev_lag_1,
    # EMA_2_Lag_2 = EMA_2 * Rev_lag_2,
    # EMA_13_Lag_1 = EMA_13 * Rev_lag_1,
    # EMA_13_Lag_2 = EMA_13 * Rev_lag_2,
    # EMA_5_Lag_1 = EMA_5 * Rev_lag_1,
    # EMA_5_Lag_2 = EMA_5 * Rev_lag_2,
    # EMA_7_Lag_1 = EMA_7 * Rev_lag_1,
    # EMA_7_Lag_2 = EMA_7 * Rev_lag_2,
    # EMA_9_Lag_1 = EMA_9 * Rev_lag_1,
    # EMA_9_Lag_2 = EMA_9 * Rev_lag_2,
    #       month_january = as.numeric(lubridate::month(Date) == 1),
    # month_february = as.numeric(lubridate::month(Date) == 2),
    # month_march = as.numeric(lubridate::month(Date) == 3),
    # month_april = as.numeric(lubridate::month(Date) == 4),
    # month_may = as.numeric(lubridate::month(Date) == 5),
    # month_june = as.numeric(lubridate::month(Date) == 6),
    # month_july = as.numeric(lubridate::month(Date) == 7),
    # month_august = as.numeric(lubridate::month(Date) == 8),
    # month_september = as.numeric(lubridate::month(Date) == 9),
    # month_october = as.numeric(lubridate::month(Date) == 10),
    # month_november = as.numeric(lubridate::month(Date) == 11),
    # month_december = as.numeric(lubridate::month(Date) == 12),
    Month = lubridate::month(Date),
    Quarter = lubridate::quarter(Date),
     Year = lubridate::year(Date),
    Week_of_Year = isoweek(Date),
    Day_of_year = yday(Date),
    Outlier_indicator = as.numeric(diff < quantile(diff, 0.05) | diff > quantile(diff, 0.95)),
     Adjacent_holiday = adj_holid2
        ) %>% filter(!is.na(Rev_lag_1) & !is.na(Rev_lag_7)  & !is.na(SD_13)
           # !is.na(Rolling_mean_2) & !is.na(Rolling_mean_13)  & 
           )


test2 <- model_data_tbl2 %>% filter(Date >= ymd("2023-09-08"))
train2 <- model_data_tbl2 %>% filter(Date < ymd("2023-09-08"))


### Modelling
  
  #initiate h2o
  h2o.init(max_mem_size = "16G")
h2o.no_progress()

# response variable
y <- "diff"

# predictors set: remove response variable and date from the set
x <- setdiff(names(train2 %>% as.h2o()), c(y, "Date"))

DL_diff <-
  h2o.automl(
    x = x,
    y = y,
    training_frame     = as.h2o(train2),
    nfolds             = 0,
     validation_frame   = test2 %>% as.h2o(),
                    include_algos = c("DeepLearning"),
    stopping_metric    = "RMSE",
    stopping_rounds    = 10,
    stopping_tolerance = 0.005,
    max_runtime_secs   = 120,
    sort_metric       = "RMSE",
    seed               = 1975
 )
DL_diff_lead <- DL_diff@leader
perf_dl_test <- h2o.performance(DL_diff_lead, newdata = as.h2o(test2))
predictions <- as.vector(h2o.predict(DL_diff_lead, as.h2o(test2))$predict)
actual_values_diff <- as.data.frame(test2) %>%
  select(Date, diff)

actual_vs_predicted_diff <- cbind(actual_values_diff, DL = predictions) %>% dplyr::rename(Actual = diff)


plot_dif <-actual_vs_predicted_diff %>%
 plot_ly() %>% 
    add_lines(x = ~ Date, y = ~ Actual, name = 'Actual values', line = list(color = 'darkblue')) %>% 
    
    add_lines(x = ~ Date, y = ~ DL, name = 'Deep Learning', 
              line = list(dash = 'dot', color = 'red')) %>% 
     layout(
    title = list(text = '', font = list(size = 16, color = 'black')),
    yaxis = list(title = list(text = 'Nr. of days', font = list(size = 14, color = 'black'))),
    xaxis = list(title = list(text = ''), font = list(size = 14)),
    legend = list(orientation = 'h', font = list(size = 12))
  )
r_squared <- as.numeric(rsq_trad(actual_vs_predicted_diff, Actual, DL)$.estimate)
plot_cor_dif <- ggplotly(
  ggplot(actual_vs_predicted_diff, aes(x = Actual, y = DL)) +
  geom_point() +  # Point plot
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "darkred") +  # Dashed diagonal line
  labs(title = "",
       x = "Actual Values",
       y = "Predicted Values") + theme_tq() + scale_x_continuous(limits = c(-10, 10)) + scale_y_continuous(limits = c(-10, 10)) +  annotate("text", x = min(actual_vs_predicted_diff$Actual), y = max(actual_vs_predicted_diff$DL), 
           label = paste("R-squared =", round(r_squared, 3)), vjust = 1, hjust = 0)
)
```
